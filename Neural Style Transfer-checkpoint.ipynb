{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec512835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (2.16.2)\n",
      "Requirement already satisfied: pillow in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (9.4.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.2 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.2->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\ritik gangwar\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "759d6980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0383514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining as function to load and preprocess an image\n",
    "def load_img(path_img):\n",
    "    max_dim = 512;\n",
    "    img = Image.open(path_img)\n",
    "    long = max(img.size)\n",
    "    scale = max_dim/long\n",
    "    img = img.resize((round(img.size[0]*scale),round(img.size[1]*scale)),Image.ANTIALIAS)\n",
    "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ac0eac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to convert tensor to image\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor*255\n",
    "    tensor = np.array(tensor,dtype=np.uint8)\n",
    "    if np.ndim(tensor)>3:\n",
    "        tensor = tensor[0]\n",
    "    return Image.fromarray(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd7a7030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to preprocess an image for VGG19\n",
    "def preprocess_image(image):\n",
    "    image=tf.keras.applications.vgg19.preprocess_input(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ebc06b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to deprocess an image\n",
    "def deprocess_image(processed_img):\n",
    "    x = processed_img.copy()\n",
    "    if len(x.shape)==4:\n",
    "        x = np.squeeze(x,0)\n",
    "    x[:,:,0]=103.939\n",
    "    x[:,:,1]=116.779\n",
    "    x[:,:,2]=123.68\n",
    "    x=x[:,:,::-1]\n",
    "    x=np.clip(x,0,255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "715537e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Loading the vgg model\n",
    "vgg=tf.keras.applications.VGG19(include_top=False,weights='imagenet')\n",
    "vgg.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a31ebdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define content and style layers\n",
    "content_layers = ['block5_conv2']\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block4_conv1',\n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26b00576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get the model and the style/content outputs\n",
    "def vgg_layers(layer_names):\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model\n",
    "\n",
    "style_extractor = vgg_layers(style_layers)\n",
    "content_extractor = vgg_layers(content_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a5fcd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the style and content outputs\n",
    "def get_style_and_content_outputs(image):\n",
    "    image = preprocess_image(image)\n",
    "    style_outputs = style_extractor(image)\n",
    "    content_outputs = content_extractor(image)\n",
    "    style_outputs = [style_layer for style_layer in style_outputs]\n",
    "    content_outputs = [content_layer for content_layer in content_outputs]\n",
    "    return {'content': content_outputs, 'style': style_outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c2d0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the Gram matrix\n",
    "def gram_matrix(input_tensor):\n",
    "    channels = int(input_tensor.shape[-1])\n",
    "    a = tf.reshape(input_tensor, [-1, channels])\n",
    "    n = tf.shape(a)[0]\n",
    "    gram = tf.matmul(a, a, transpose_a=True)\n",
    "    return gram / tf.cast(n, tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35633100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the style loss\n",
    "def style_loss(style_output, style_target):\n",
    "    style_output_gram = gram_matrix(style_output)\n",
    "    style_target_gram = gram_matrix(style_target)\n",
    "    return tf.reduce_mean(tf.square(style_output_gram - style_target_gram))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19b64c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate the content loss\n",
    "def content_loss(content_output, content_target):\n",
    "    return tf.reduce_mean(tf.square(content_output - content_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6aa07081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):\n",
    "    style_weight, content_weight = loss_weights\n",
    "    \n",
    "    model_outputs = model(init_image)\n",
    "    \n",
    "    style_output_features = model_outputs['style']\n",
    "    content_output_features = model_outputs['content']\n",
    "    \n",
    "    style_score = 0\n",
    "    content_score = 0\n",
    "    \n",
    "    weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "    for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "        style_score += weight_per_style_layer * style_loss(comb_style[0], target_style)\n",
    "    \n",
    "    weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "    for target_content, comb_content in zip(content_features, content_output_features):\n",
    "        content_score += weight_per_content_layer * content_loss(comb_content[0], target_content)\n",
    "    \n",
    "    style_score *= style_weight\n",
    "    content_score *= content_weight\n",
    "    \n",
    "    loss = style_score + content_score\n",
    "    return loss, style_score, content_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a46e3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ritik Gangwar\\AppData\\Local\\Temp\\ipykernel_23684\\1631255852.py:7: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  img = img.resize((round(img.size[0]*scale),round(img.size[1]*scale)),Image.ANTIALIAS)\n"
     ]
    }
   ],
   "source": [
    "# Load the content and style images\n",
    "content_image_path = 'C:/Users/Ritik Gangwar/OneDrive/Desktop/ML Projects/Hair Style Recommendation System/FaceShape Dataset/testing_set/Heart/heart (5).jpg'\n",
    "style_image_path = 'C:/Users/Ritik Gangwar/OneDrive/Desktop/anime.jpg'\n",
    "\n",
    "content_image = load_img(content_image_path)\n",
    "style_image = load_img(style_image_path)\n",
    "\n",
    "content_image = tf.convert_to_tensor(content_image, dtype=tf.float32)\n",
    "style_image = tf.convert_to_tensor(style_image, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c01f3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the content and style feature representations\n",
    "content_targets = content_extractor(preprocess_image(content_image))\n",
    "style_targets = style_extractor(preprocess_image(style_image))\n",
    "gram_style_features = [gram_matrix(style_target) for style_target in style_targets]\n",
    "\n",
    "# Initialize the target image with the content image\n",
    "init_image = tf.Variable(content_image, dtype=tf.float32)\n",
    "\n",
    "# Set the style and content weights\n",
    "style_weight = 1e-4\n",
    "content_weight = 1e4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e59e9949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the optimizer\n",
    "optimizer = tf.optimizers.Adam(learning_rate=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f274f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training step\n",
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, style_score, content_score = compute_loss(vgg_layers(style_layers + content_layers), [style_weight, content_weight], image, gram_style_features, content_targets)\n",
    "    grad = tape.gradient(loss, image)\n",
    "    optimizer.apply_gradients([(grad, image)])\n",
    "    image.assign(tf.clip_by_value(image, 0.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aefed9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Ritik Gangwar\\AppData\\Local\\Temp\\ipykernel_23684\\2460650448.py\", line 5, in train_step  *\n        loss, style_score, content_score = compute_loss(vgg_layers(style_layers + content_layers), [style_weight, content_weight], image, gram_style_features, content_targets)\n    File \"C:\\Users\\Ritik Gangwar\\AppData\\Local\\Temp\\ipykernel_23684\\5580974.py\", line 7, in compute_loss  *\n        style_output_features = model_outputs['style']\n\n    TypeError: list indices must be integers or slices, not str\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps_per_epoch):\n\u001b[0;32m     12\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 13\u001b[0m     train_step(init_image)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain step: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(step))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\RITIKG~1\\AppData\\Local\\Temp\\__autograph_generated_filee1mkzwkh.py:9\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mFunctionScope(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_step\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mConversionOptions(recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, optional_features\u001b[38;5;241m=\u001b[39m(), internal_convert_user_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)) \u001b[38;5;28;01mas\u001b[39;00m fscope:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m----> 9\u001b[0m         loss, style_score, content_score \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(compute_loss), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(vgg_layers), (ag__\u001b[38;5;241m.\u001b[39mld(style_layers) \u001b[38;5;241m+\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(content_layers),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), [ag__\u001b[38;5;241m.\u001b[39mld(style_weight), ag__\u001b[38;5;241m.\u001b[39mld(content_weight)], ag__\u001b[38;5;241m.\u001b[39mld(image), ag__\u001b[38;5;241m.\u001b[39mld(gram_style_features), ag__\u001b[38;5;241m.\u001b[39mld(content_targets)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     10\u001b[0m     grad \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(loss), ag__\u001b[38;5;241m.\u001b[39mld(image)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     11\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(optimizer)\u001b[38;5;241m.\u001b[39mapply_gradients, ([(ag__\u001b[38;5;241m.\u001b[39mld(grad), ag__\u001b[38;5;241m.\u001b[39mld(image))],), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[1;32mC:\\Users\\RITIKG~1\\AppData\\Local\\Temp\\__autograph_generated_filexlx26edy.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__compute_loss\u001b[1;34m(model, loss_weights, init_image, gram_style_features, content_features)\u001b[0m\n\u001b[0;32m     10\u001b[0m style_weight, content_weight \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(loss_weights)\n\u001b[0;32m     11\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model), (ag__\u001b[38;5;241m.\u001b[39mld(init_image),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 12\u001b[0m style_output_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(model_outputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m content_output_features \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(model_outputs)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     14\u001b[0m style_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Ritik Gangwar\\AppData\\Local\\Temp\\ipykernel_23684\\2460650448.py\", line 5, in train_step  *\n        loss, style_score, content_score = compute_loss(vgg_layers(style_layers + content_layers), [style_weight, content_weight], image, gram_style_features, content_targets)\n    File \"C:\\Users\\Ritik Gangwar\\AppData\\Local\\Temp\\ipykernel_23684\\5580974.py\", line 7, in compute_loss  *\n        style_output_features = model_outputs['style']\n\n    TypeError: list indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "# Run the optimization\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(init_image)\n",
    "        print(\".\", end='')\n",
    "    print(\"Train step: {}\".format(step))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end - start))\n",
    "\n",
    "# Convert and save the final output image\n",
    "final_image = tensor_to_image(init_image)\n",
    "final_image.save(\"output.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac569ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
